{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stk_FAupS7OW",
        "outputId": "8463dca5-298a-4352-d718-ebe8441f1dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adj_rus (1).npy: Shape=(189216, 2), Type=<class 'numpy.ndarray'>\n",
            "edge_feat_rus.npy: Shape=(189216, 63), Type=<class 'numpy.ndarray'>\n",
            "label_bi_rus (1).npy: Shape=(189216,), Type=<class 'numpy.ndarray'>\n",
            "node_random (1).npy: Shape=(19374,), Type=<class 'numpy.ndarray'>\n",
            "adj_random_list.dict (1): Loaded successfully, Type=<class 'collections.defaultdict'>\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# 불러올 파일 리스트\n",
        "files = [\n",
        "     \"adj_rus (1).npy\", \"edge_feat_rus.npy\",\n",
        "    \"label_bi_rus (1).npy\",  \"node_random (1).npy\"\n",
        "]\n",
        "\n",
        "# 각 파일 로드 및 shape 확인\n",
        "for file in files:\n",
        "    try:\n",
        "        data = np.load(f\"{file}\", allow_pickle=True)\n",
        "        print(f\"{file}: Shape={data.shape}, Type={type(data)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "# adj_random_list.dict 파일 로드\n",
        "try:\n",
        "    with open(f\"adj_random_list (1).dict\", \"rb\") as f:\n",
        "        adj_random_list = pickle.load(f)\n",
        "    print(f\"adj_random_list.dict (1): Loaded successfully, Type={type(adj_random_list)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading adj_random_list(1).dict: {e}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# Load Data\n",
        "# ---------------------------\n",
        "edge_feat = np.load(\"edge_feat_rus.npy\", allow_pickle=True)  # Shape: (3540241, 77)\n",
        "adj = np.load(\"adj_rus (1).npy\", allow_pickle=True)  # Shape: (3540241, 2)\n",
        "label = np.load(\"label_bi_rus (1).npy\")  # Shape: (3540241,)"
      ],
      "metadata": {
        "id": "ByV8ZfVAVM-j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"edge_feat shape:\", edge_feat.shape)\n",
        "print(\"adj shape:\", adj.shape)\n",
        "print(\"label shape:\", label.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6SlMM1hVRhb",
        "outputId": "28f9d262-e9ff-4908-f06a-e9bf9c589aa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edge_feat shape: (189216, 63)\n",
            "adj shape: (189216, 2)\n",
            "label shape: (189216,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6W2pudMS7OX",
        "outputId": "6dac3449-2bcb-4f04-eb93-9ce6761fb51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 107batch [00:03, 32.02batch/s, loss=0.0137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Avg Loss: 0.002389, Accuracy: 0.9634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 107batch [00:03, 33.80batch/s, loss=0.0084]                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Avg Loss: 0.001061, Accuracy: 0.9905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 107batch [00:02, 37.22batch/s, loss=0.0107]                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Avg Loss: 0.001040, Accuracy: 0.9909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 107batch [00:03, 34.68batch/s, loss=0.0127]                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Avg Loss: 0.000987, Accuracy: 0.9910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 107batch [00:03, 31.08batch/s, loss=0.0065]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Avg Loss: 0.000945, Accuracy: 0.9908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 107batch [00:03, 34.37batch/s, loss=0.0096]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Avg Loss: 0.000935, Accuracy: 0.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 107batch [00:03, 34.79batch/s, loss=0.0073]                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Avg Loss: 0.000983, Accuracy: 0.9909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 107batch [00:03, 33.54batch/s, loss=0.0065]                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Avg Loss: 0.000926, Accuracy: 0.9909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 107batch [00:03, 33.20batch/s, loss=0.0057]                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Avg Loss: 0.000944, Accuracy: 0.9909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 107batch [00:03, 34.52batch/s, loss=0.0108]                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Avg Loss: 0.000945, Accuracy: 0.9910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Ensure edge_feat contains only numerical values\n",
        "edge_feat = edge_feat.astype(np.float32)  # Convert to float32\n",
        "\n",
        "# Reduce Dataset Size: Sample 5% of Data\n",
        "subset_size = 0.05  # Use only 5% of data\n",
        "sample_indices, _ = train_test_split(np.arange(len(label)), train_size=subset_size, stratify=label, random_state=42)\n",
        "\n",
        "# Apply sampling\n",
        "edge_feat = edge_feat[sample_indices]\n",
        "label = label[sample_indices]\n",
        "\n",
        "# ---------------------------\n",
        "# Fix Adjacency Index Mapping Issue\n",
        "# ---------------------------\n",
        "# Find unique nodes that remain in the sampled dataset\n",
        "valid_nodes = set(np.unique(edge_feat))  # Nodes that exist in feature matrix\n",
        "node_mapping = {node: i for i, node in enumerate(valid_nodes)}\n",
        "\n",
        "# Re-map adjacency list indices to match the reduced dataset\n",
        "adj_remapped = []\n",
        "for src, dst in adj[sample_indices]:\n",
        "    if src in node_mapping and dst in node_mapping:  # Only keep valid edges\n",
        "        adj_remapped.append([node_mapping[src], node_mapping[dst]])\n",
        "\n",
        "# Convert to NumPy array and PyTorch tensor\n",
        "adj_numeric = np.array(adj_remapped, dtype=np.int64)\n",
        "adj_numeric = torch.tensor(adj_numeric, dtype=torch.long)\n",
        "\n",
        "# Convert Labels to One-Hot Encoding (Multi-Label)\n",
        "num_classes = int(label.max()) + 1  # Get number of unique classes\n",
        "label_one_hot = np.eye(num_classes)[label.astype(int)]  # Convert to one-hot encoding\n",
        "label = torch.tensor(label_one_hot, dtype=torch.float32)  # Convert to PyTorch tensor\n",
        "\n",
        "# Convert Node Features to PyTorch Tensor\n",
        "edge_feat = torch.tensor(edge_feat, dtype=torch.float32)\n",
        "\n",
        "# ---------------------------\n",
        "# GNN Model Definition\n",
        "# ---------------------------\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats, dropout=0.5):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = nn.Linear(in_feats, hidden_feats)\n",
        "        self.conv2 = nn.Linear(hidden_feats, out_feats)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.conv1(x)\n",
        "        h = F.relu(h)\n",
        "\n",
        "        # Fix: Ensure edge_index is 2D\n",
        "        if edge_index.dim() == 1:\n",
        "            edge_index = edge_index.view(-1, 2)  # Reshape into (num_edges, 2)\n",
        "\n",
        "        edge_src, edge_dst = edge_index[:, 0], edge_index[:, 1]\n",
        "\n",
        "        # Fix: Ensure valid_mask is never empty\n",
        "        valid_mask = (edge_dst < h.shape[0]) & (edge_src < h.shape[0])  # Ensure indices are valid\n",
        "\n",
        "        if valid_mask.sum() == 0:  # If all edges are invalid, return h unchanged\n",
        "            return torch.sigmoid(self.conv2(F.dropout(h, p=self.dropout, training=self.training)))\n",
        "\n",
        "        # Fix: Ensure index_add_ does not fail\n",
        "        h_scatter = torch.zeros_like(h)\n",
        "        h_scatter.index_add_(0, edge_dst[valid_mask], h[edge_src[valid_mask]])\n",
        "\n",
        "        h = F.dropout(h_scatter, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h)\n",
        "        return torch.sigmoid(h)  # Sigmoid activation for multi-label classification\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize Model, Loss, Optimizer\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_features = edge_feat.shape[1]  # 77 features\n",
        "num_classes = label.shape[1]  # Correct way to get the number of labels\n",
        "\n",
        "model = GNN(in_feats=num_features, hidden_feats=128, out_feats=num_classes).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# ---------------------------\n",
        "# Train-Test Split on Reduced Dataset\n",
        "# ---------------------------\n",
        "train_idx, test_idx = train_test_split(np.arange(len(label)), test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=42)\n",
        "\n",
        "# Move tensors to GPU if available\n",
        "edge_feat, adj_numeric, label = edge_feat.to(device), adj_numeric.to(device), label.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# Training Loop\n",
        "# ---------------------------\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Use tqdm for batch progress tracking\n",
        "    with tqdm(total=len(train_idx) // batch_size, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
        "        for i in range(0, len(train_idx), batch_size):\n",
        "            batch_idx = train_idx[i:i + batch_size]\n",
        "            batch_feat = edge_feat  # Use full node feature matrix\n",
        "            batch_adj = adj_numeric  # Adjacency is global, not batch-specific\n",
        "            batch_label = label[batch_idx]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_feat, batch_adj)\n",
        "            loss = criterion(outputs[batch_idx], batch_label)  # Select only batch outputs\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Compute Accuracy\n",
        "            predicted_labels = (outputs[batch_idx] > 0.5).float()  # Convert to binary labels\n",
        "            correct_predictions += (predicted_labels == batch_label).sum().item()\n",
        "            total_samples += batch_label.numel()  # Total number of elements in labels\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Compute and print epoch accuracy\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Loss: {epoch_loss / len(train_idx):.6f}, Accuracy: {epoch_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ8_FGMFS7OY",
        "outputId": "6ca03e28-0da6-41bc-c67e-3966b473ae1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Validation Set ---\n",
            "Validation Loss: 0.0505\n",
            "Validation Accuracy: 0.9934\n",
            "Validation Cohen's Kappa: 0.9868\n",
            "Validation AUC: 0.9906\n",
            "\n",
            "--- Test Set ---\n",
            "Test Loss: 0.0496\n",
            "Test Accuracy: 0.9926\n",
            "Test Cohen's Kappa: 0.9852\n",
            "Test AUC: 0.9931\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1022\n",
            "           1       0.99      0.99      0.99       870\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1892\n",
            "   macro avg       0.99      0.99      0.99      1892\n",
            "weighted avg       0.99      0.99      0.99      1892\n",
            " samples avg       0.99      0.99      0.99      1892\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, cohen_kappa_score, roc_auc_score\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation on Validation and Test Set\n",
        "# ---------------------------\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Validation\n",
        "    val_outputs = model(edge_feat, adj_numeric)\n",
        "    val_outputs = val_outputs[val_idx]  # Select only validation samples\n",
        "    val_labels = label[val_idx]\n",
        "\n",
        "    val_loss = criterion(val_outputs, val_labels)  # Compute validation loss\n",
        "\n",
        "    val_outputs_binary = (val_outputs > 0.5).float()  # Threshold for metrics\n",
        "\n",
        "    # Test\n",
        "    test_outputs = model(edge_feat, adj_numeric)\n",
        "    test_outputs = test_outputs[test_idx]  # Select only test samples\n",
        "    test_labels = label[test_idx]\n",
        "\n",
        "    test_loss = criterion(test_outputs, test_labels)  # Compute test loss\n",
        "\n",
        "    test_outputs_binary = (test_outputs > 0.5).float()  # Threshold for metrics\n",
        "\n",
        "# ---------------------------\n",
        "# Validation Metrics\n",
        "# ---------------------------\n",
        "val_true = val_labels.cpu().numpy()\n",
        "val_pred = val_outputs_binary.cpu().numpy()\n",
        "val_outputs_proba = val_outputs.cpu().numpy()\n",
        "\n",
        "val_accuracy = accuracy_score(val_true.flatten(), val_pred.flatten())\n",
        "val_kappa = cohen_kappa_score(val_true.flatten(), val_pred.flatten())\n",
        "val_auc = roc_auc_score(val_true.flatten(), val_outputs_proba.flatten())\n",
        "\n",
        "# ---------------------------\n",
        "# Test Metrics\n",
        "# ---------------------------\n",
        "test_true = test_labels.cpu().numpy()\n",
        "test_pred = test_outputs_binary.cpu().numpy()\n",
        "test_outputs_proba = test_outputs.cpu().numpy()\n",
        "\n",
        "test_accuracy = accuracy_score(test_true.flatten(), test_pred.flatten())\n",
        "test_kappa = cohen_kappa_score(test_true.flatten(), test_pred.flatten())\n",
        "test_auc = roc_auc_score(test_true.flatten(), test_outputs_proba.flatten())\n",
        "\n",
        "# ---------------------------\n",
        "# Print Results\n",
        "# ---------------------------\n",
        "print(\"\\n--- Validation Set ---\")\n",
        "print(f\"Validation Loss: {val_loss.item():.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Validation Cohen's Kappa: {val_kappa:.4f}\")\n",
        "print(f\"Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(\"\\n--- Test Set ---\")\n",
        "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Cohen's Kappa: {test_kappa:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(test_true, test_pred, zero_division=0))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}